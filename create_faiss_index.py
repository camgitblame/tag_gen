import pandas as pd
import numpy as np
import faiss
import json
from sentence_transformers import SentenceTransformer

# === Config ===
DATA_PATH = "movies_metadata.csv"
INDEX_PATH = "faiss_overview.index"
TEXT_LIST_PATH = "id_to_text.json"
EMBED_MODEL = "all-MiniLM-L6-v2"

# === Step 1: Load and clean overviews ===
print("ğŸ“‚ Loading data...")
df = pd.read_csv(DATA_PATH)
df = df[df["overview"].notna()]
df = df[df["overview"].str.strip() != ""]
overviews = df["overview"].drop_duplicates().tolist()
print(f"âœ… Loaded {len(overviews)} unique overviews.")

# === Step 2: Embed with SentenceTransformer ===
print(f"ğŸ” Encoding with {EMBED_MODEL}...")
encoder = SentenceTransformer(EMBED_MODEL)
embeddings = encoder.encode(overviews, show_progress_bar=True)
embeddings = np.array(embeddings).astype(np.float32)

# === Step 3: Build FAISS index ===
print("ğŸ§  Building FAISS index...")
dim = embeddings.shape[1]
index = faiss.IndexFlatL2(dim)
index.add(embeddings)

# === Step 4: Save index and overview list ===
print(f"ğŸ’¾ Saving index to {INDEX_PATH}...")
faiss.write_index(index, INDEX_PATH)

print(f"ğŸ’¾ Saving overview list to {TEXT_LIST_PATH}...")
with open(TEXT_LIST_PATH, "w") as f:
    json.dump(overviews, f)

print("âœ… Done! FAISS index and overview list saved.")
